# ReverseEngineeringNetworks

Reverse engineering deep ReLU networks is a critical problem in understanding the complex behavior and interpretability of neural networks. In this research, we present a novel method for reconstructing deep ReLU networks by leveraging con- vex optimization techniques and a sampling-based approach. Our method begins by sampling points in the input space and querying the black box model to obtain the corresponding hyperplanes. We then define a convex optimization problem with carefully chosen constraints and conditions to guarantee its convexity. The ob- jective function is designed to minimize the discrepancy between the reconstructed network’s output and the target model’s output, subject to the constraints. We employ gradient descent to optimize the objective function, incorporating L1 or L2 regularization as needed to encourage sparse or smooth solutions. Our research contributes to the growing body of work on reverse engineering deep ReLU net- works and paves the way for new advancements in neural network interpretability and security.

In this work, we propose a novel approach to reverse engineering deep ReLU networks by formulating an optimization problem and employing gradient descent to solve it. We build upon insights from previous works on the complexity of linear regions and activation patterns in deep ReLU networks, the expressive power of deep neural networks, and methods for model reconstruction from model explanations. Our approach aims to advance the understanding of deep ReLU networks and provide a practical method for reconstructing them from limited information.
